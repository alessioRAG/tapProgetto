networks:
  Alessio:
    external: true
    name: Alessio
    ipam:
      driver: default
      config:
        - subnet: 10.0.100.0/24

services:
  python-app:
    build:
      context: ./python
    volumes:
      - ./python:/app
    command: python /app/scraping.py
    networks:
      - Alessio

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 2181:2181
    networks:
      Alessio:
        ipv4_address: 10.0.100.22
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:latest
    hostname: kafka
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      Alessio:
        ipv4_address: 10.0.100.23
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    hostname: kafka_ui
    ports:
      - 8080:8080
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=PLAINTEXT://kafka:9092
    networks:
      - Alessio

  init_kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint:
    - sh
    - -c
    - |
        echo "Waiting for Kafka to be ready..." && \
        sleep 10 && \
        kafka-topics --delete --bootstrap-server kafka:9092 --topic datiSquadre || echo "Topic non esistente, continuando..." && \
        kafka-topics --delete --bootstrap-server kafka:9092 --topic datiGiocatori || echo "Topic non esistente, continuando..." && \
        kafka-topics --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic datiSquadre && \
        kafka-topics --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic datiGiocatori && \
        echo "Topic 'datiCalcio' created."
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    network_mode: "service:kafka"

  logstash:
    image: docker.elastic.co/logstash/logstash:8.13.2
    hostname: logstash
    volumes:
      - ./logstash/pipeline/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./logstash/dati:/dati
      - ./python/datiSerieA:/datiPythonSerieA
    stdin_open: true 
    tty: true
    environment:
      XPACK_MONITORING_ENABLED: "false"
    networks:
      - Alessio

  elasticsearch:
    hostname: elasticsearch
    container_name: elasticsearch2
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    ports:
      - "9200:9200"
    networks:
      Alessio:
        ipv4_address: 10.0.100.24
    mem_limit: 3GB

  spark:
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark
    container_name: spark2
    restart: always
    volumes:
      - ./spark/code:/code
      - ./spark/dataset:/dataset
    command: > 
        /opt/spark/bin/spark-submit --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.elasticsearch:elasticsearch-spark-30_2.12:8.13.4  /code/main_spark.py
    ports:
      - "4040:4040"
    depends_on:
      - kafka
      - elasticsearch
    networks:
      - Alessio

  spark-ml:
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-ml
    container_name: spark-ml2
    restart: on-failure
    depends_on:
      - spark
    volumes:
      - ./spark/code:/code
      - ./spark/dataset:/dataset
    ports:
      - "4041:4040"
    networks:
      - Alessio

  kibana:
    hostname: kibana
    container_name: kibana2
    image: docker.elastic.co/kibana/kibana:8.13.4
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    volumes:
      - ./kibana:/scripts
    networks:
      - Alessio